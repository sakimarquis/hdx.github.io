---
permalink: /
title: "Hi! I am Huadong"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



I use models to understand behaviors of both biological and artificial agents and explore how computation could be implemented in neural networks. Following the release of GPT-4, my research interest has partially shifted towards understanding the emergence of intelligence within large language models. 

My current research interests include:

- Use recurrent neural network as cognitive model to identify the hidden variables in decision-making across species.
- Examine the ways prior information could be encoded in working memory system through optimization-based neural network models.
- Investigate how humans utilize language as a tool of control and agency through dialogue. 

# About me

I am a first year PhD student in the Cognition and Neural Systems Program at the University of Arizona. I work with [Robert Wilson](https://scholar.google.com/citations?user=4LxikSIAAAAJ&hl=en&oi=ao). Prior to my PhD, I worked with [Xue-Xin Wei](https://scholar.google.com/citations?user=7Pd1QzwAAAAJ&hl=en&oi=ao) at UT Austin, and [Da-Hui Wang](https://scholar.google.com/citations?user=6BkFUZcAAAAJ&hl=en&oi=sra) at Beijing Normal University. Before that, I used EEG to study working memory. Before that, I studied counseling psychology with a focus on cognitive behavioral therapy.

The pronunciation of my name is "HWAH-doang SHAWNG".

## Some of my writing

I am addicted to explore complex ideas. I write some my *controversial* thoughts on how we understand the world around us. See **Blog Posts**. I hope you find these ideas interesting! Feel free to share your thoughts or comments with me via email—I'd love to hear your perspective.

## Some interesting facts:

- I enjoyed reading when I was young. My favorite writers are James Joyce, Milan Kundera, Jorge Borges, Franz Kafka , Dostoyevsky and Edgar Allan Poe. 
- This stupid username was set when I was a teenager, came from [Saki](https://en.wikipedia.org/wiki/Saki) and [Márquez](https://en.wikipedia.org/wiki/Gabriel_Garc%C3%ADa_M%C3%A1rquez).
- Yet I didn't read much after high school. Suddenly lose my patience with long books. 
- I am addicted to computer games, but only when there is an exam approaching. Since there are few exams I should take, I seldom play them now. 
- I enjoy skiing, sick jokes and embarrassing short videos. 
- Using a second language is painful for me, mostly because I can't help being sarcastic but I can't do it well in English. 
- I am bad at calculating. I often mess up with single digit calculations, even with a pen and paper. This always makes me doubt myself as a researcher in computational neuroscience.  (But large language models also fail to do simple calculations, I am not alone)



This website will be slowly updating...



#### Thank you for reading this far, and here I'd like to make two calls to action:

<small>1. **Avoid publishing papers on APA** – I have never successfully downloaded a paper from APA. It’s frustratingly inconvenient, and I believe it creates unnecessary barriers to accessing research. We need better, more accessible platforms to share knowledge and collaborate in academia.</small>

<small>2. **Stop doing parameter recovery in any computational studies** – From an epistemological standpoint, parameter recovery is not only absurd but also largely pointless. More importantly, it carries an air of authority that lacks real justification. which discourages innovation. It makes researchers more inclined to stick with established models rather than develop new ones. The fear of having to justify a model through parameter recovery often suppresses the exploration of more creative or unconventional approaches. In chaotic systems, parameter recovery is simply impossible. Yet, when reviewers encounter novel models they don’t fully understand, all they can do is say something reflexively, “Please perform parameter recovery; otherwise, your model is meaningless.” We can forgive this classical conditioning—after all, I suspect my advisor’s Ten Simple Rules paper may have contributed to this trend. But the fact remains: this process is incredibly time-consuming. The key point here is that the best way to evaluate a model is through nested cross-validation. Among all the competing models, the one with the best fit should be preserved and respected—it doesn't need to justify itself through additional proofs. From a Bayesian perspective, a model improving fitting performance is the best Bayesian explanation. </small>

